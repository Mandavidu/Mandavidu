{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mandavidu/Mandavidu/blob/main/Cloud_Computing_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYU4aTkKvoFV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PBfef8LLljZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "c66dca8b-7224-4587-c8cc-7a0b1ed6dffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 0 B/110 kB 0%] [Connected to cloud.r-project.org \r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [810 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,695 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,242 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.1 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,975 kB]\n",
            "Fetched 9,465 kB in 2s (4,044 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79aae833e710>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8eb54a010fa2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Word count**"
      ],
      "metadata": {
        "id": "ruZrLCfar33f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Read data from n-grams.txt\n",
        "with open(\"/content/sample_data/n-grams.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Convert text to lowercase of n-grams.txt\n",
        "text_lower = text.lower()\n",
        "\n",
        "# Remove punctuation marks of n-grams.txt\n",
        "text_cleaned = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text_cleaned.split()\n",
        "\n",
        "# Generate n-grams for n = 1 to 5\n",
        "ngrams = {}\n",
        "for n in range(1, 6):\n",
        "    ngrams[n] = [(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "# Count occurrences of n-grams\n",
        "ngram_counts = {}\n",
        "for n, ngram_list in ngrams.items():\n",
        "    ngram_counts[n] = Counter([tuple(ngram) for ngram in ngram_list])\n",
        "\n",
        "# finaly print the 5 most frequent n-grams for each value of n\n",
        "for n, counts in ngram_counts.items():\n",
        "    print(f\"======5 most frequent {n}-grams======\")\n",
        "    print(\"index count ngram\")\n",
        "    for i, (ngram, count) in enumerate(counts.most_common(5), start=1):\n",
        "        ngram_str = \" \".join(ngram)\n",
        "        print(f\"{i}. {count} '{ngram_str}'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UYuWDnZVrs-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdde911c-6ad5-4fc9-d4ca-c23ab3cee96a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======5 most frequent 1-grams======\n",
            "index count ngram\n",
            "1. 5 'to'\n",
            "2. 3 'care'\n",
            "3. 2 'you'\n",
            "4. 2 'needs'\n",
            "5. 2 'we'\n",
            "======5 most frequent 2-grams======\n",
            "index count ngram\n",
            "1. 1 'to know'\n",
            "2. 1 'know you'\n",
            "3. 1 'you is'\n",
            "4. 1 'is to'\n",
            "5. 1 'to love'\n",
            "======5 most frequent 3-grams======\n",
            "index count ngram\n",
            "1. 1 'to know you'\n",
            "2. 1 'know you is'\n",
            "3. 1 'you is to'\n",
            "4. 1 'is to love'\n",
            "5. 1 'to love you'\n",
            "======5 most frequent 4-grams======\n",
            "index count ngram\n",
            "1. 1 'to know you is'\n",
            "2. 1 'know you is to'\n",
            "3. 1 'you is to love'\n",
            "4. 1 'is to love you'\n",
            "5. 1 'to love you hello'\n",
            "======5 most frequent 5-grams======\n",
            "index count ngram\n",
            "1. 1 'to know you is to'\n",
            "2. 1 'know you is to love'\n",
            "3. 1 'you is to love you'\n",
            "4. 1 'is to love you hello'\n",
            "5. 1 'to love you hello world'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Serializing problem**\n",
        "\n",
        "Since UDFs in PySpark can be inefficient because they require serialization of data between JVM and Python.The apply method uses (\"F.udf\") to register the (\"is_stopword\") function as a UDF directly which is not allowed in PySpark. PySpark UDFs need to be serialized and sent to the worker nodes, so they cannot reference Python functions defined outside of the UDF."
      ],
      "metadata": {
        "id": "DyQRHmrhsPCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from dataclasses import dataclass\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class StopWordFilter:\n",
        "    stop_words: List[str]\n",
        "\n",
        "    def apply(self, df: DataFrame) -> DataFrame:\n",
        "        stop_words = self.stop_words\n",
        "\n",
        "        def is_stopword(word: str) -> bool:\n",
        "            return word in stop_words\n",
        "\n",
        "        df = df.withColumn(\n",
        "            \"is_stop_word\",\n",
        "            F.udf(is_stopword, returnType=T.BooleanType())(\"word\"))\n",
        "        return df\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "    # Create a DataFrame\n",
        "    data = [(\"Alice\",), (\"Bob\",), (\"BadBoy\",)]\n",
        "    df = spark.createDataFrame(data, schema=['word'])\n",
        "\n",
        "    # initialize the class\n",
        "    stop_word_filter = StopWordFilter([\"BadBoy\"])\n",
        "\n",
        "    # apply the filter\n",
        "    df = stop_word_filter.apply(df)\n",
        "    df.show()"
      ],
      "metadata": {
        "id": "tLc6Ef1so2Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf1fc9d-0a7d-424d-bedd-c3d6d15c1aea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  word|is_stop_word|\n",
            "+------+------------+\n",
            "| Alice|       false|\n",
            "|   Bob|       false|\n",
            "|BadBoy|        true|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution:**\n",
        "\n",
        "To resolve the above problem, I made (\"is_stopword\") function compatible with PySpark UDFs. This can be done by using a lambda function or a regular python function defined inside the apply method. I use a lambda function inside the apply method and modified the code as follows:"
      ],
      "metadata": {
        "id": "AeLrppISXnoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from dataclasses import dataclass\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class StopWordFilter:\n",
        "    stop_words: List[str]\n",
        "\n",
        "    def apply(self, df: DataFrame) -> DataFrame:\n",
        "        stop_words = self.stop_words\n",
        "\n",
        "        # Define a lambda function for stop word filtering\n",
        "        is_stopword = F.udf(lambda word: word in stop_words, T.BooleanType())\n",
        "\n",
        "        # Apply the lambda function to create a new column\n",
        "        df = df.withColumn(\"is_stop_word\", is_stopword(\"word\"))\n",
        "        return df\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "    # Create a DataFrame\n",
        "    data = [(\"Alice\",), (\"Bob\",), (\"BadBoy\",)]\n",
        "    df = spark.createDataFrame(data, schema=['word'])\n",
        "\n",
        "    # initialize the class\n",
        "    stop_word_filter = StopWordFilter([\"BadBoy\"])\n",
        "\n",
        "    # apply the filter\n",
        "    df = stop_word_filter.apply(df)\n",
        "    df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn1JxQ4wEVUZ",
        "outputId": "587cbe38-aeaa-4cb6-b1a8-5271e234b6b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  word|is_stop_word|\n",
            "+------+------------+\n",
            "| Alice|       false|\n",
            "|   Bob|       false|\n",
            "|BadBoy|        true|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Page Rank**\n",
        "PageRank is an algorithm used by Google to rank web pages in its search engine results."
      ],
      "metadata": {
        "id": "BWXtoca75G8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def page_rank(input_file, num_iterations, damping_factor):\n",
        "    # Read the input file and extract necessary information\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    num_pages = len(lines) - 2\n",
        "\n",
        "    # Create a directed graph using NetworkX.\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for i in range(1, num_pages + 1):\n",
        "        G.add_node(str(i))\n",
        "\n",
        "    # Add edges to the graph\n",
        "    for line in lines[2:]:\n",
        "        src, *destinations = map(str.strip, line.split())\n",
        "        for dest in destinations:\n",
        "            G.add_edge(dest, src)  # Reverse the direction to match the example\n",
        "\n",
        "    # Calculate PageRank using the given formula by professor\n",
        "    pr = {}\n",
        "    for page in G.nodes:\n",
        "        pr[page] = (1 - damping_factor)  # Initialize with (1 - d)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        new_pr = {}\n",
        "        for page in G.nodes:\n",
        "            new_pr[page] = (1 - damping_factor)  # Initialize with (1 - d)\n",
        "            for predecessor in G.predecessors(page):\n",
        "                new_pr[page] += pr[predecessor] / len(list(G.successors(predecessor))) * damping_factor\n",
        "                 # PR(p) = (1 - d) + d * (PR(p1)/L(p1) + PR(p2)/L(p2) + ... + PR(pk)/L(pk))\n",
        "\n",
        "        pr = new_pr\n",
        "\n",
        "    return pr\n",
        "\n",
        "def print_page_rank(pr):\n",
        "    # Find the page with the highest PageRank value\n",
        "    highest_page = max(pr, key=pr.get)\n",
        "    highest_pr_value = pr[highest_page]\n",
        "    # Sort the pages by PageRank value\n",
        "    sorted_pages = sorted(pr.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the page with the highest PageRank value first\n",
        "    print(f\"Page {highest_page}: has higher rank = {highest_pr_value}\")\n",
        "\n",
        "    # Print the 5 pages with the highest PageRank values\n",
        "    for i, (page, pr_value) in enumerate(sorted_pages[:5], 1):\n",
        "        print(f\"Page {page}: has rank: = {pr_value}\")\n",
        "\n",
        "input_file = \"/content/sample_data/input_1.txt\"\n",
        "num_iterations = 100  # Increased iterations\n",
        "damping_factor = 0.85  # Damping factor as given in ppt\n",
        "result = page_rank(input_file, num_iterations, damping_factor)\n",
        "print_page_rank(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0OmWNLzTZj",
        "outputId": "519c7dce-18d3-4962-973c-18e3ba4332cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1: has higher rank = 1.918918764179665\n",
            "Page 1: has rank: = 1.918918764179665\n",
            "Page 2: has rank: = 0.6936936461331438\n",
            "Page 3: has rank: = 0.6936936461331438\n",
            "Page 4: has rank: = 0.6936936461331438\n",
            "Page 5: has rank: = 0.15000000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Linear Regression Model**"
      ],
      "metadata": {
        "id": "4tBynPyTscY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Step 1: Convert all the features (without medv) into a single column\n",
        "#         Call this new vector column as 'Attributes' in the outputCol\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LinearRegression\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "data = spark.read.csv(\"/content/sample_data/BostonHousing.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Target column\n",
        "target_column = 'medv'\n",
        "\n",
        "# Get the list of feature columns (all columns except the target column)\n",
        "feature_columns = [col for col in data.columns if col != target_column]\n",
        "\n",
        "# Create a vector assembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_columns,\n",
        "    outputCol=\"Attributes\")\n",
        "\n",
        "# Transform the data to incorporate the assembled features\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Step 3: Create a Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"Attributes\", labelCol=target_column)\n",
        "\n",
        "# Fit the model to the training data\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Show predictions\n",
        "#predictions.select(\"medv\", target_column, *feature_columns).show()\n",
        "\n",
        "# Step 4: Analyze the model statistically\n",
        "# Evaluate the model by importing RegressionEvaluator module\n",
        "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"mse\")\n",
        "evaluation_summary = lr_model.evaluate(test_data)\n",
        "print(\"Root Mean Squared Error (RMSE):\", evaluation_summary.rootMeanSquaredError)\n",
        "mse = evaluator.evaluate(predictions)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "\n",
        "print(\"R-squared:\", evaluation_summary.r2)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiBhFfK7QWms",
        "outputId": "11f7fad3-dbbb-438c-a4b8-a98a34cc78ac"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 3.952468980908394\n",
            "Mean Squared Error (MSE): 15.622011045043038\n",
            "Mean Absolute Error (MAE): 2.9764433568995194\n",
            "R-squared: 0.8024391787661578\n"
          ]
        }
      ]
    }
  ]
}